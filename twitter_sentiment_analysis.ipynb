{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "# data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the dataset\n",
    "data = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.703060e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/2015 11:35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/2015 11:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/2015 11:15</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/2015 11:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/2015 11:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  5.703060e+17           neutral                        1.0000   \n",
       "1  5.703010e+17          positive                        0.3486   \n",
       "2  5.703010e+17           neutral                        0.6837   \n",
       "3  5.703010e+17          negative                        1.0000   \n",
       "4  5.703010e+17          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "     tweet_created tweet_location               user_timezone  \n",
       "0  2/24/2015 11:35            NaN  Eastern Time (US & Canada)  \n",
       "1  2/24/2015 11:15            NaN  Pacific Time (US & Canada)  \n",
       "2  2/24/2015 11:15      Lets Play  Central Time (US & Canada)  \n",
       "3  2/24/2015 11:15            NaN  Pacific Time (US & Canada)  \n",
       "4  2/24/2015 11:14            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viewing the first 5 rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence',\n",
       "       'negativereason', 'negativereason_confidence', 'airline',\n",
       "       'airline_sentiment_gold', 'name', 'negativereason_gold',\n",
       "       'retweet_count', 'text', 'tweet_coord', 'tweet_created',\n",
       "       'tweet_location', 'user_timezone'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# importing nltk libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "n = len(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,n):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', data['text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    stopword = stopwords.words('english')\n",
    "    stopword.remove('not')\n",
    "    review = [lemm.lemmatize(word) for word in review if word not in stopword]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "x = cv.fit_transform(corpus).toarray()\n",
    "y = data[\"airline_sentiment\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11712"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2928"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "\n",
    "# fitting the data\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative' 'negative' 'negative' ... 'negative' 'positive' 'negative'] ['negative' 'negative' 'negative' ... 'negative' 'negative' 'negative']\n"
     ]
    }
   ],
   "source": [
    "# predicitng the resukts\n",
    "y_pred = model.predict(x_test)\n",
    "print(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[873 372 625]\n",
      " [120 210 284]\n",
      " [ 81  58 305]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.47404371584699456"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting if the review is positive, negative or neutral\n",
    "def reviews(new_review):\n",
    "    new_review = re.sub('[^a-zA-Z]', ' ', new_review)\n",
    "    new_review = new_review.lower()\n",
    "    new_review = new_review.split()\n",
    "    stopword = stopwords.words('english')\n",
    "    stopword.remove('not')\n",
    "    new_review = [lemm.lemmatize(word) for word in new_review if word not in stopword]\n",
    "    new_review = ' '.join(new_review)\n",
    "    new_corpus = [new_review]\n",
    "    new_x_test = cv.transform(new_corpus).toarray()\n",
    "    new_y_pred = model.predict(new_x_test)\n",
    "    return new_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive'], dtype='<U8')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews('American airlines is the best airline in the US')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The review was predicted correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive'], dtype='<U8')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews('the southwest airlines fly to dallas everyday')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The review was incorrectly predicted.The review was neutral but predicted as positive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive'], dtype='<U8')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews('I never fly in american airlines since its not good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The review was predicted incorrectly as positive instead of negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Implementing using Deep Learning model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y = encoder.transform(y)\n",
    "\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_y)\n",
    "print(dummy_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, dummy_y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the ANN model\n",
    "model1 = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 200)               2514800   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 603       \n",
      "=================================================================\n",
      "Total params: 2,555,603\n",
      "Trainable params: 2,555,603\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# adding first layer\n",
    "model1.add(tf.keras.layers.Dense(units=200, input_dim=12573, activation='relu'))\n",
    "# adding second layer\n",
    "model1.add(tf.keras.layers.Dense(units=200, activation='relu'))\n",
    "# output layer\n",
    "model1.add(tf.keras.layers.Dense(units=3, activation='softmax'))\n",
    "# compiling the model\n",
    "model1.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11712 samples\n",
      "Epoch 1/50\n",
      "11712/11712 [==============================] - 4s 376us/sample - loss: 0.6055 - acc: 0.7499\n",
      "Epoch 2/50\n",
      "11712/11712 [==============================] - 4s 344us/sample - loss: 0.3031 - acc: 0.8880\n",
      "Epoch 3/50\n",
      "11712/11712 [==============================] - 4s 346us/sample - loss: 0.1429 - acc: 0.9503\n",
      "Epoch 4/50\n",
      "11712/11712 [==============================] - 4s 347us/sample - loss: 0.0631 - acc: 0.9798\n",
      "Epoch 5/50\n",
      "11712/11712 [==============================] - 4s 348us/sample - loss: 0.0342 - acc: 0.9897\n",
      "Epoch 6/50\n",
      "11712/11712 [==============================] - 4s 346us/sample - loss: 0.0262 - acc: 0.9925\n",
      "Epoch 7/50\n",
      "11712/11712 [==============================] - 4s 346us/sample - loss: 0.0215 - acc: 0.9927\n",
      "Epoch 8/50\n",
      "11712/11712 [==============================] - 4s 346us/sample - loss: 0.0172 - acc: 0.9937\n",
      "Epoch 9/50\n",
      "11712/11712 [==============================] - 5s 390us/sample - loss: 0.0151 - acc: 0.9941\n",
      "Epoch 10/50\n",
      "11712/11712 [==============================] - 4s 354us/sample - loss: 0.0147 - acc: 0.9939\n",
      "Epoch 11/50\n",
      "11712/11712 [==============================] - 4s 361us/sample - loss: 0.0134 - acc: 0.9944\n",
      "Epoch 12/50\n",
      "11712/11712 [==============================] - 4s 351us/sample - loss: 0.0132 - acc: 0.9944\n",
      "Epoch 13/50\n",
      "11712/11712 [==============================] - 4s 343us/sample - loss: 0.0123 - acc: 0.9947\n",
      "Epoch 14/50\n",
      "11712/11712 [==============================] - 4s 344us/sample - loss: 0.0124 - acc: 0.9947\n",
      "Epoch 15/50\n",
      "11712/11712 [==============================] - 4s 344us/sample - loss: 0.0122 - acc: 0.9944\n",
      "Epoch 16/50\n",
      "11712/11712 [==============================] - 4s 344us/sample - loss: 0.0116 - acc: 0.9947\n",
      "Epoch 17/50\n",
      "11712/11712 [==============================] - 4s 343us/sample - loss: 0.0113 - acc: 0.9948\n",
      "Epoch 18/50\n",
      "11712/11712 [==============================] - 4s 345us/sample - loss: 0.0110 - acc: 0.9950\n",
      "Epoch 19/50\n",
      "11712/11712 [==============================] - 4s 344us/sample - loss: 0.0135 - acc: 0.9942\n",
      "Epoch 20/50\n",
      "11712/11712 [==============================] - 4s 344us/sample - loss: 0.0144 - acc: 0.9933\n",
      "Epoch 21/50\n",
      "11712/11712 [==============================] - 4s 345us/sample - loss: 0.0215 - acc: 0.9922\n",
      "Epoch 22/50\n",
      "11712/11712 [==============================] - 4s 344us/sample - loss: 0.0163 - acc: 0.9938\n",
      "Epoch 23/50\n",
      "11712/11712 [==============================] - 4s 344us/sample - loss: 0.0112 - acc: 0.9946\n",
      "Epoch 24/50\n",
      "11712/11712 [==============================] - 4s 344us/sample - loss: 0.0098 - acc: 0.9953\n",
      "Epoch 25/50\n",
      "11712/11712 [==============================] - 4s 343us/sample - loss: 0.0095 - acc: 0.9951\n",
      "Epoch 26/50\n",
      "11712/11712 [==============================] - 4s 343us/sample - loss: 0.0093 - acc: 0.9949\n",
      "Epoch 27/50\n",
      "11712/11712 [==============================] - 4s 344us/sample - loss: 0.0093 - acc: 0.9949\n",
      "Epoch 28/50\n",
      "11712/11712 [==============================] - 4s 345us/sample - loss: 0.0092 - acc: 0.9956\n",
      "Epoch 29/50\n",
      "11712/11712 [==============================] - 4s 345us/sample - loss: 0.0093 - acc: 0.9949\n",
      "Epoch 30/50\n",
      "11712/11712 [==============================] - 4s 344us/sample - loss: 0.0090 - acc: 0.9953\n",
      "Epoch 31/50\n",
      "11712/11712 [==============================] - 4s 345us/sample - loss: 0.0104 - acc: 0.9948\n",
      "Epoch 32/50\n",
      "11712/11712 [==============================] - 4s 344us/sample - loss: 0.0116 - acc: 0.9945\n",
      "Epoch 33/50\n",
      "11712/11712 [==============================] - 4s 344us/sample - loss: 0.0120 - acc: 0.9940\n",
      "Epoch 34/50\n",
      "11712/11712 [==============================] - 4s 344us/sample - loss: 0.0139 - acc: 0.9945\n",
      "Epoch 35/50\n",
      "11712/11712 [==============================] - 4s 346us/sample - loss: 0.0089 - acc: 0.9954\n",
      "Epoch 36/50\n",
      "11712/11712 [==============================] - 4s 346us/sample - loss: 0.0088 - acc: 0.9952\n",
      "Epoch 37/50\n",
      "11712/11712 [==============================] - 4s 344us/sample - loss: 0.0088 - acc: 0.9950\n",
      "Epoch 38/50\n",
      "11712/11712 [==============================] - 4s 354us/sample - loss: 0.0086 - acc: 0.9953\n",
      "Epoch 39/50\n",
      "11712/11712 [==============================] - 4s 347us/sample - loss: 0.0084 - acc: 0.9950\n",
      "Epoch 40/50\n",
      "11712/11712 [==============================] - 4s 344us/sample - loss: 0.0090 - acc: 0.9952\n",
      "Epoch 41/50\n",
      "11712/11712 [==============================] - 4s 345us/sample - loss: 0.0087 - acc: 0.9951\n",
      "Epoch 42/50\n",
      "11712/11712 [==============================] - 4s 352us/sample - loss: 0.0087 - acc: 0.9950\n",
      "Epoch 43/50\n",
      "11712/11712 [==============================] - 4s 371us/sample - loss: 0.0086 - acc: 0.9950\n",
      "Epoch 44/50\n",
      "11712/11712 [==============================] - 4s 347us/sample - loss: 0.0085 - acc: 0.9953\n",
      "Epoch 45/50\n",
      "11712/11712 [==============================] - 4s 350us/sample - loss: 0.0084 - acc: 0.9953\n",
      "Epoch 46/50\n",
      "11712/11712 [==============================] - 4s 350us/sample - loss: 0.0085 - acc: 0.9956\n",
      "Epoch 47/50\n",
      "11712/11712 [==============================] - 4s 344us/sample - loss: 0.0103 - acc: 0.9952\n",
      "Epoch 48/50\n",
      "11712/11712 [==============================] - 4s 349us/sample - loss: 0.0090 - acc: 0.9953\n",
      "Epoch 49/50\n",
      "11712/11712 [==============================] - 4s 347us/sample - loss: 0.0157 - acc: 0.9943\n",
      "Epoch 50/50\n",
      "11712/11712 [==============================] - 4s 347us/sample - loss: 0.0133 - acc: 0.9943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f17d014db10>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model\n",
    "model1.fit(X_train, y_train, batch_size = 32, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting if the review is positive, negative or neutral using deep learning model\n",
    "def reviews1(new_review):\n",
    "    new_review = re.sub('[^a-zA-Z]', ' ', new_review)\n",
    "    new_review = new_review.lower()\n",
    "    new_review = new_review.split()\n",
    "    stopword = stopwords.words('english')\n",
    "    stopword.remove('not')\n",
    "    new_review = [lemm.lemmatize(word) for word in new_review if word not in stopword]\n",
    "    new_review = ' '.join(new_review)\n",
    "    new_corpus = [new_review]\n",
    "    new_x_test = cv.transform(new_corpus).toarray()\n",
    "    new_y_pred = model1.predict(new_x_test)\n",
    "    x = new_y_pred.round()\n",
    "    if x[0][0] == 1:\n",
    "        print(\"The sentiment of the review is Negative\")\n",
    "    elif x[0][2] == 1:\n",
    "        print(\"The sentiment of the review is Positive\")\n",
    "    else:\n",
    "        print(\"The sentiment of the review is Neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the review is Positive\n"
     ]
    }
   ],
   "source": [
    "reviews1('American airlines is the best airline in the US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the review is Neutral\n"
     ]
    }
   ],
   "source": [
    "reviews1('the southwest airlines fly to dallas everyday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the review is Negative\n"
     ]
    }
   ],
   "source": [
    "reviews1('I never fly in american airlines since its not good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviews were correctly predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
